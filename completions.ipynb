{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt messages sent to OpenAI API:[{'role': 'assistant', 'content': 'Hello! How can I help you today?'}]\n",
      "Hello! How can I assist you today?"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Stream' object has no attribute 'model_dump'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 99\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Update database and save it to the right json file\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m completions\u001b[38;5;241m.\u001b[39mappend(completion\u001b[38;5;241m.\u001b[39mmodel_dump())\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m    101\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(completions,json_file)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Stream' object has no attribute 'model_dump'"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Set parameters\n",
    "\n",
    "name = \"1\"\n",
    "model = \"gpt-3.5-turbo\"               # important options: \"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"\n",
    "loops = 1\n",
    "memory = 1\n",
    "stable_system_message = False\n",
    "assistant_transform = False    \n",
    "assistant_transform_type = \"user\"     # options: \"user\", \"user-assistant\", \"system\"\n",
    "stream = True\n",
    "\n",
    "first_prompt = [\n",
    "  #{\"role\": \"system\", \"content\": \"\"},\n",
    "  {\"role\": \"user\", \"content\": \"\"}\n",
    "]\n",
    "\n",
    "# Choose or create the right folder with json files\n",
    "\n",
    "folder_name = f\"{model} memory-{memory}\"\n",
    "if assistant_transform:\n",
    "  folder_name += f\" {assistant_transform_type}\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "for loop in range(loops):\n",
    "\n",
    "  # Load completions from json, if continuing with a file generated in the past\n",
    "\n",
    "  try:\n",
    "    with open(f\"{folder_name}/{name}.json\", \"r\") as json_file:\n",
    "      completions=json.load(json_file)\n",
    "\n",
    "  # If not, create a new completions database\n",
    "\n",
    "  except FileNotFoundError:\n",
    "    completions = []\n",
    "\n",
    "  # Collect messages generated so far\n",
    "\n",
    "  messages = first_prompt\n",
    "  for completion in completions:\n",
    "      message = completion[\"choices\"][0][\"message\"]\n",
    "      messages.append({\"role\": message[\"role\"], \"content\": message[\"content\"]}   \n",
    "      )\n",
    "\n",
    "  # Pick prompt messages\n",
    "\n",
    "  if memory >= len(messages):\n",
    "      prompt_messages = messages\n",
    "  else:\n",
    "      #print(f\"Memory smaller than the entire conversation. Forgetting {len(messages)-memory} messages\")\n",
    "      prompt_messages = []\n",
    "      if stable_system_message:\n",
    "        prompt_messages.append(first_prompt[0])\n",
    "        print(prompt_messages)\n",
    "        print(\"stable_system_message\")\n",
    "      prompt_messages = prompt_messages+messages[(len(messages)-memory):]\n",
    "\n",
    "  # Transform assistant role to assistant and user, user, or system roles\n",
    "\n",
    "  if assistant_transform:\n",
    "    if assistant_transform_type == \"user-assistant\":\n",
    "      i = 1\n",
    "      while i <= len(prompt_messages):\n",
    "        if prompt_messages[-i][\"role\"] == \"assistant\":\n",
    "          prompt_messages[-i][\"role\"] = \"user\"\n",
    "        i += 2\n",
    "    else:\n",
    "      for prompt_message in prompt_messages:\n",
    "        if prompt_message[\"role\"] == \"assistant\":\n",
    "          prompt_message[\"role\"] = assistant_transform_type\n",
    "\n",
    "\n",
    "  # Send to OpenAI API to get a new completion\n",
    "\n",
    "  completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages = prompt_messages,\n",
    "        stream = stream,\n",
    "      )\n",
    "  print(f\"Prompt messages sent to OpenAI API:{prompt_messages}\")\n",
    "  if stream:\n",
    "    all_completion_data = []\n",
    "    content_string = \"\"\n",
    "    for chunk in completion:\n",
    "      whole_completion.append(chunk)\n",
    "      print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
    "      content += chunk.choices[0].delta.content\n",
    "    completion = all_completion_data[0]\n",
    "    completion.choices[0].delta.content = content\n",
    "    normalized_choices = pd.json_normalize(completion, record_path=['choices','delta'], record_prefix='')\n",
    "    completion.choices = normalized_choices\n",
    "  else:\n",
    "    print(completion.choices[0].message.content)\n",
    "\n",
    "  # Update database and save it to the right json file\n",
    "\n",
    "  completions.append(completion.model_dump())\n",
    "  with open(f\"{folder_name}/{name}.json\", \"w\") as json_file:\n",
    "      json.dump(completions,json_file)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionChunk(id='chatcmpl-9rVqFsodmLisXKz5pYW67GSXOsexL', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1722540811, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None),\n",
       " ChatCompletionChunk(id='chatcmpl-9rVqFsodmLisXKz5pYW67GSXOsexL', choices=[Choice(delta=ChoiceDelta(content='Hello', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1722540811, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None),\n",
       " ChatCompletionChunk(id='chatcmpl-9rVqFsodmLisXKz5pYW67GSXOsexL', choices=[Choice(delta=ChoiceDelta(content='!', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1722540811, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None),\n",
       " ChatCompletionChunk(id='chatcmpl-9rVqFsodmLisXKz5pYW67GSXOsexL', choices=[Choice(delta=ChoiceDelta(content=' How', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1722540811, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None),\n",
       " ChatCompletionChunk(id='chatcmpl-9rVqFsodmLisXKz5pYW67GSXOsexL', choices=[Choice(delta=ChoiceDelta(content=' can', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1722540811, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None),\n",
       " ChatCompletionChunk(id='chatcmpl-9rVqFsodmLisXKz5pYW67GSXOsexL', choices=[Choice(delta=ChoiceDelta(content=' I', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1722540811, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None),\n",
       " ChatCompletionChunk(id='chatcmpl-9rVqFsodmLisXKz5pYW67GSXOsexL', choices=[Choice(delta=ChoiceDelta(content=' assist', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1722540811, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None),\n",
       " ChatCompletionChunk(id='chatcmpl-9rVqFsodmLisXKz5pYW67GSXOsexL', choices=[Choice(delta=ChoiceDelta(content=' you', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1722540811, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None),\n",
       " ChatCompletionChunk(id='chatcmpl-9rVqFsodmLisXKz5pYW67GSXOsexL', choices=[Choice(delta=ChoiceDelta(content=' today', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1722540811, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None),\n",
       " ChatCompletionChunk(id='chatcmpl-9rVqFsodmLisXKz5pYW67GSXOsexL', choices=[Choice(delta=ChoiceDelta(content='?', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1722540811, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None),\n",
       " ChatCompletionChunk(id='chatcmpl-9rVqFsodmLisXKz5pYW67GSXOsexL', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1722540811, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai.Stream object at 0x12d939b80>\n"
     ]
    }
   ],
   "source": [
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__next__', '__orig_bases__', '__orig_class__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__stream__', '__subclasshook__', '__weakref__', '_cast_to', '_client', '_decoder', '_iter_events', '_iterator', 'close', 'response']\n"
     ]
    }
   ],
   "source": [
    "print(dir(completion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo memory-1/1.json\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hello! How can I assist you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hello! How can I assist you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hello! How can I assist you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hello! How can I assist you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hello! How can I help you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Display data\n",
    "\n",
    "# Load the JSON file\n",
    "\n",
    "file_path = f\"{folder_name}/{name}.json\"\n",
    "\n",
    "#file_path = f\"gpt-3.5-turbo memory-1 user/2.json\"\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Normalize the JSON data in completions\n",
    "\n",
    "normalized_data = pd.json_normalize(data, record_path='choices', meta=[\n",
    "    'id', 'created', 'model', 'object', 'service_tier', 'system_fingerprint', \n",
    "    ['usage', 'completion_tokens'], ['usage', 'prompt_tokens'], ['usage', 'total_tokens']], sep='.',\n",
    "    meta_prefix='', record_prefix='choices.')\n",
    "\n",
    "# Make more important columns more visible\n",
    "\n",
    "normalized_data = normalized_data[['choices.message.content', 'model', \n",
    "       'usage.completion_tokens', 'usage.prompt_tokens', 'usage.total_tokens', 'id', \n",
    "       'choices.finish_reason', 'choices.index', 'choices.logprobs', \n",
    "       'choices.message.role', 'choices.message.function_call', 'choices.message.tool_calls', \n",
    "       'created', 'object', 'service_tier', 'system_fingerprint'\n",
    "       ]]\n",
    "\n",
    "# Display\n",
    "\n",
    "content_all = normalized_data[\"choices.message.content\"]\n",
    "last = content_all.iloc[-1]\n",
    "\n",
    "print(file_path)\n",
    "print(len(content_all))\n",
    "\n",
    "for content in content_all:\n",
    "   display_markdown(content, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-9rSyNew3bRbcf5FoSt88NJ9iu4ljW',\n",
       " 'choices': [{'finish_reason': 'stop',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'message': {'content': 'Some categories I can assist with include (but are not limited to):\\n- General knowledge\\n- Science\\n- Technology\\n- History\\n- Health\\n- Literature\\n- Geography\\n- Mathematics\\n- Language and linguistics\\n\\nFeel free to ask me anything within these categories or any other topics you may be curious about!',\n",
       "    'role': 'assistant',\n",
       "    'function_call': None,\n",
       "    'tool_calls': None}}],\n",
       " 'created': 1722529783,\n",
       " 'model': 'gpt-3.5-turbo-0125',\n",
       " 'object': 'chat.completion',\n",
       " 'service_tier': None,\n",
       " 'system_fingerprint': None,\n",
       " 'usage': {'completion_tokens': 64, 'prompt_tokens': 43, 'total_tokens': 107}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completions[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
